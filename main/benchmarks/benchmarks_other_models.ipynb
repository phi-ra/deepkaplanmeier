{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarks\n",
        "\n",
        "Contains benchmark simulations with other models. This was outsourced to google\n",
        "colab to make it accessible to the whole community. For the applications, it requires access the the data, change the path accordingly to data in your drive or simply upload the data to your runtime. \n",
        "\n",
        "Note, the notebook was run on cpu but depending on your data you might want to \n",
        "use a GPU. This might change the value of the benchmarks slightly"
      ],
      "metadata": {
        "id": "cb27T0QmgVVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch torchtuples pycox lifelines matplotlib"
      ],
      "metadata": {
        "id": "mrnSH75UAVfJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages (need both tensorflow and torch)\n",
        "import pickle\n",
        "\n",
        "# Wrangling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import gamma\n",
        "import re\n",
        "from collections import namedtuple\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Backends\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "import torchtuples as tt\n",
        "\n",
        "# Models and metrics\n",
        "from pycox.evaluation import EvalSurv\n",
        "from pycox.models import LogisticHazard, PMF, DeepHitSingle, MTLR"
      ],
      "metadata": {
        "id": "oMADE89lkQsN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_preds(output_raw, y_surv_, y_cens_):\n",
        "    output_ = (pd.DataFrame(list(map(np.ravel, output_raw)))\n",
        "                                .reset_index()\n",
        "                                .rename(columns={'index':'timeline'}))\n",
        "\n",
        "    time_idx = np.array(output_['timeline'].astype(float))\n",
        "    output_ = output_.set_index(time_idx).drop(columns='timeline')\n",
        "\n",
        "    evaluation_results = EvalSurv(output_,\n",
        "                                    y_surv_,\n",
        "                                    y_cens_,\n",
        "                                    censor_surv='km')\n",
        "    \n",
        "    linspace_brier = np.linspace(y_surv_.min(), \n",
        "                                 y_surv_.max(), \n",
        "                                 100)\n",
        "    \n",
        "    \n",
        "    concordance_idx = evaluation_results.concordance_td('antolini')\n",
        "    brier_idx = evaluation_results.integrated_brier_score(linspace_brier) \n",
        "\n",
        "    return concordance_idx, brier_idx"
      ],
      "metadata": {
        "id": "ntojfO-fkgbG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2EyqQtf2tIK7"
      },
      "outputs": [],
      "source": [
        "class DeepKaplanMeier:\n",
        "    \"\"\"\n",
        "    Deep-Kaplan-Meier estimator class.\n",
        "\n",
        "\n",
        "    Parameters\n",
        "        ----------\n",
        "        periods: int\n",
        "            Total number of periods that observation has taken place\n",
        "\n",
        "        weights_max: float, optional (default=0)\n",
        "            Maximum add to weights for losses. This parameter is optional \n",
        "            and can be used in the case heavy censoring is present. In turn\n",
        "            it will give more weight to losses corresponding to later periods\n",
        "\n",
        "        learning_rate: float, optional (default=0.005)\n",
        "            The learning rate used in the optimization\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 periods=None, \n",
        "                 weights_max = 0, \n",
        "                 learning_rate=0.005):\n",
        "        \"\"\"\n",
        "        ToDo: Make period counter more flexible (include between period counters)\n",
        "        \"\"\"\n",
        "        self._set_periods(periods)\n",
        "        self.weights_max = weights_max\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "    def _set_periods(self,\n",
        "                     periods):\n",
        "        assert isinstance(periods, int), f\"\"\"Specify the periods as integer,\n",
        "                                             you specified it as {type(periods)}\"\"\"\n",
        "        self.periods = periods\n",
        "\n",
        "    def prepare_survival(self, survival_time, censoring):\n",
        "        \"\"\"\n",
        "        Prepares target input matrix for estimation\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        survival time: int\n",
        "            Indicator of last observed period, either censoring time or\n",
        "            failure time\n",
        "            \n",
        "        censoring: bool\n",
        "            Censoring indicator, if True observation is censored\n",
        "            \n",
        "        returns: y-matrix and weights-matrix\n",
        "        \"\"\"\n",
        "        if isinstance(survival_time, list):\n",
        "            observations = len(survival_time)\n",
        "        elif isinstance(survival_time, np.ndarray):\n",
        "            observations = survival_time.shape[0]\n",
        "        else:\n",
        "            raise Exception(f\"\"\"you need to provide either an array or\n",
        "                                list of survival times,\n",
        "                                you provided {type(survival_time)}\"\"\")\n",
        "                \n",
        "        weight_matrix = np.ones(shape=(observations, self.periods+1))\n",
        "        survival_matrix = pad_sequences( [np.repeat(1, self.periods + 1)] + \n",
        "                                         [np.repeat(1, surv_time+1) if cens_ else\n",
        "                                         np.repeat(1, surv_time) for\n",
        "                                         surv_time, cens_ in zip(survival_time, censoring)],\n",
        "                                        padding='post')[1:,:]\n",
        "\n",
        "        weight_matrix[censoring,:] = weight_matrix[censoring,:] * survival_matrix[censoring,:]\n",
        "\n",
        "        return survival_matrix, weight_matrix\n",
        "\n",
        "    def create_output_struct(self, \n",
        "                             hidden_layer_output,\n",
        "                             activation,\n",
        "                             preoutput_units):\n",
        "        \"\"\"\n",
        "        ToDo: make static (set periods in input rather than from class)\n",
        "        Helper function to create connected output units according to \n",
        "        a given parametrization. Main function is compile_model\n",
        "        \"\"\"\n",
        "        outputs = []\n",
        "        if preoutput_units is not None:\n",
        "            for _ in range(self.periods + 1):\n",
        "                aggregation_layer = tf.keras.layers.Dense(units=preoutput_units, \n",
        "                                                        activation=activation)(hidden_layer_output)\n",
        "                output_unit = tf.keras.layers.Dense(1, activation='sigmoid')(aggregation_layer)\n",
        "                outputs.append(output_unit)\n",
        "        else:\n",
        "            for _ in range(self.periods + 1):\n",
        "                output_unit = tf.keras.layers.Dense(1, activation='sigmoid')(hidden_layer_output)\n",
        "                outputs.append(output_unit)\n",
        "\n",
        "        for unit_idx in np.arange(1, self.periods + 1):\n",
        "            outputs[unit_idx] = tf.keras.layers.Multiply()([outputs[unit_idx], outputs[unit_idx - 1]])\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "    @staticmethod\n",
        "    def create_rep_struct(input_mod,\n",
        "                          hidden_units,\n",
        "                          activation):\n",
        "        \"\"\"\n",
        "        Helper function to create embedding layer for the hidden\n",
        "        features. Main function is compile_model\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(hidden_units, int):\n",
        "            input_mod = tf.keras.layers.Dense(hidden_units, \n",
        "                              activation=activation)(input_mod)\n",
        "        elif isinstance(hidden_units, list):\n",
        "            for units_ in hidden_units:\n",
        "                input_mod = tf.keras.layers.Dense(units_, \n",
        "                              activation=activation)(input_mod)  \n",
        "        else:\n",
        "            raise TypeError('hidden_units must be int or list or ints')\n",
        "\n",
        "        return input_mod\n",
        "\n",
        "    def compile_model(self,\n",
        "                      input_shape,\n",
        "                      hidden_units,\n",
        "                      activation='relu',\n",
        "                      preoutput=50):\n",
        "        \"\"\"\n",
        "        Compiles keras model according to inputs\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_shape: tuple\n",
        "            Tuple specifying input shape. In general of the form (k, )\n",
        "\n",
        "        hidden_units: int or list(int)\n",
        "            If integer, the number of hidden units in the single hidden layer\n",
        "            If list, one integers specifying hidden units per hidden layer\n",
        "\n",
        "        activation: str, optional (default='relu')\n",
        "            activation function for hidden units\n",
        "\n",
        "        preoutput: int, optional (default=50)\n",
        "            Number of hidden units in the final layer when combining the weights\n",
        "            from the embedding layer\n",
        "        \"\"\"\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "        input_layer = tf.keras.Input(shape=input_shape)\n",
        "        input_mod = input_layer\n",
        "\n",
        "        input_layer = tf.keras.Input(shape=input_shape)\n",
        "        input_mod = input_layer\n",
        "\n",
        "        input_mod = self.create_rep_struct(input_mod, hidden_units, activation)\n",
        "        \n",
        "        outputs = self.create_output_struct(hidden_layer_output=input_mod,\n",
        "                                            activation=activation,\n",
        "                                            preoutput_units=preoutput)\n",
        "\n",
        "        self.model = tf.keras.Model(inputs=input_layer, outputs=outputs)\n",
        "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
        "                           loss=\"binary_crossentropy\",\n",
        "                           loss_weights=np.array(1 + np.linspace(0,\n",
        "                                                self.weights_max,36))\n",
        "                           )\n",
        "\n",
        "    def return_comp_graph_raw(self,\n",
        "                              input_shape,\n",
        "                              hidden_units,\n",
        "                              activation='relu', \n",
        "                              preoutput=None):\n",
        "        \"\"\"\n",
        "        Returns computational graph by combining the embedding structure\n",
        "        and the output structure\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_shape: tuple\n",
        "            Tuple specifying input shape. In general of the form (k, )\n",
        "\n",
        "        hidden_units: int or list(int)\n",
        "            If integer, the number of hidden units in the single hidden layer\n",
        "            If list, one integers specifying hidden units per hidden layer\n",
        "\n",
        "        activation: str, optional (Default: 'relu')\n",
        "            activation function for hidden units\n",
        "\n",
        "        preoutput: int, optional (Default: 50)\n",
        "            Number of hidden units in the final layer when combining the weights\n",
        "            from the embedding layer\n",
        "\n",
        "\n",
        "        returns:\n",
        "            uncompiled model structure as a tf.keras.Model\n",
        "        \"\"\"\n",
        "        tf.keras.backend.clear_session()\n",
        "        input_layer = tf.keras.Input(shape=input_shape)\n",
        "        input_mod = input_layer\n",
        "\n",
        "        input_mod = self.create_rep_struct(input_mod, hidden_units, activation)\n",
        "        \n",
        "        outputs = self.create_output_struct(hidden_layer_output=input_mod,\n",
        "                                            activation=activation, \n",
        "                                            preoutput_units=preoutput)\n",
        "                                            \n",
        "        model_raw =  tf.keras.Model(inputs=input_layer, outputs=outputs)\n",
        "\n",
        "        return model_raw\n",
        "\n",
        "    def fit_model(self, X_train, y_survival, censoring,\n",
        "                  epochs=50, batch_size = 256, verbose=False):\n",
        "        \"\"\"\n",
        "        Fits the compiled model using data\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_train: np.array or tf.tensor\n",
        "            array or similar of the predictions variables, must be of the \n",
        "            dimension as specified with \"input_shape\" otherwise an error\n",
        "            will be raised\n",
        "\n",
        "        y_survival: np.array of int\n",
        "            the survival times for every row in X_train, must be specified\n",
        "            as an integer at the moment. Otherwise it must be specified during\n",
        "            the preprocessing\n",
        "\n",
        "        censoring: np.array of bool\n",
        "            vector indicating whether an observation was censored or not,\n",
        "            used to create the weighting matrix\n",
        "\n",
        "        epochs: int, optional (default=50)\n",
        "            number of epochs (passes of the training set) to be performed until\n",
        "            stop\n",
        "\n",
        "        batch_size: int, optional (default=256)\n",
        "            batch size for training operation\n",
        "\n",
        "        verbose: bool, optional (default=False)\n",
        "            whether to show training progress for every batch that is processed\n",
        "            if set to True will also return a history object that can be used\n",
        "            for plotting\n",
        "        \"\"\"\n",
        "        y_train_np, weights_train_np = self.prepare_survival(y_survival, censoring)\n",
        "        \n",
        "        y_train = [y_train_np[:, i] for i in range(y_train_np.shape[1])]\n",
        "        weights_train = [weights_train_np[:, i] for i in range(weights_train_np.shape[1])]\n",
        "        \n",
        "        if verbose:\n",
        "            hist_ = self.model.fit(X_train,\n",
        "                                   y_train,\n",
        "                                   sample_weight=weights_train,\n",
        "                                   epochs=epochs,\n",
        "                                   batch_size=batch_size)\n",
        "            return hist_\n",
        "        else:\n",
        "            self.model.fit(x=X_train,\n",
        "                           y=y_train,\n",
        "                           sample_weight=weights_train,\n",
        "                           verbose=verbose,\n",
        "                           epochs=epochs,\n",
        "                           batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applications \n"
      ],
      "metadata": {
        "id": "xYkGPoxaeYzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Application Medical"
      ],
      "metadata": {
        "id": "Vw8nX8fpegGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/data_colab/application_training_smart.csv')\n",
        "y_target = np.array(df.tevent.astype(int))\n",
        "y_censoring = np.where(df.event == 1, False, True)\n",
        "\n",
        "df_val = pd.read_csv('/content/drive/MyDrive/data_colab/application_test_smart.csv')\n",
        "y_target_val = np.array(df_val.tevent.astype(int))\n",
        "y_censoring_val = np.where(df_val.event == 1, False, True)\n",
        "y_censoring_val_eval = np.where(df_val.event == 1, 1, 0)"
      ],
      "metadata": {
        "id": "A2Ub6UCuuTtk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_periods = int(y_target.max())"
      ],
      "metadata": {
        "id": "JSLhAOLCueTQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_unscaled = df.loc[:, df.columns[3:]]\n",
        "features_unscaled_val = df_val.loc[:, df_val.columns[3:]]\n",
        "\n",
        "features_numeric = features_unscaled.max(axis=0) > 1\n",
        "\n",
        "numeric_ = features_unscaled.loc[:,features_numeric.values]\n",
        "binary_ = features_unscaled.loc[:,~features_numeric.values].to_numpy()\n",
        "\n",
        "numeric_val = features_unscaled_val.loc[:,features_numeric.values]\n",
        "binary_val = features_unscaled_val.loc[:,~features_numeric.values].to_numpy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(numeric_)\n",
        "\n",
        "numeric_trans = scaler.transform(numeric_)\n",
        "numeric_trans_val = scaler.transform(numeric_val)\n",
        "\n",
        "features = np.concatenate([numeric_trans, binary_], axis=1).astype('float32')\n",
        "features_val = np.concatenate([numeric_trans_val, binary_val], axis=1).astype('float32')"
      ],
      "metadata": {
        "id": "gMeuPO9Quf8T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model\n",
        "tf.keras.utils.set_random_seed(123)\n",
        "model_smart = DeepKaplanMeier(total_periods,\n",
        "                              learning_rate=0.05,\n",
        "                              weights_max=3.2)\n",
        "\n",
        "model_smart.compile_model((30, ),\n",
        "                          [18,6,8],\n",
        "                          preoutput=6)\n",
        "\n",
        "for repeater_epoch in range(5):\n",
        "  print(f'...this is epoch {5*repeater_epoch+5}')\n",
        "\n",
        "  tf.keras.utils.set_random_seed(123)\n",
        "  model_smart.fit_model(features,\n",
        "                      y_target.astype(int), \n",
        "                      y_censoring,\n",
        "                      epochs=5,\n",
        "                      verbose=False, \n",
        "                      batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q44n324Ed7_S",
        "outputId": "3c30659e-db03-4e18-c250-fea0cbe024a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...this is epoch 5\n",
            "...this is epoch 10\n",
            "...this is epoch 15\n",
            "...this is epoch 20\n",
            "...this is epoch 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_raw = model_smart.model.predict(features_val)\n",
        "\n",
        "linspace_brier = np.linspace(y_target_val.min(), y_target_val.max(), 100)\n",
        "score_conc, score_brier = calculate_preds(preds_raw, y_target_val, y_censoring_val_eval)\n",
        "\n",
        "print(f'concordance: {score_conc} \\nbrier: {score_brier}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK6Gt-avgcbI",
        "outputId": "b480b1ca-34b9-4a46-cbd9-d92db7879d9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 1s 4ms/step\n",
            "concordance: 0.6868355646525617 \n",
            "brier: 0.1216432980045617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_raw = model_smart.model.predict(features_val)\n",
        "\n",
        "linspace_brier = np.linspace(y_target_val.min(), y_target_val.max(), 100)\n",
        "score_conc, score_brier = calculate_preds(preds_raw, y_target_val, y_censoring_val_eval)\n",
        "\n",
        "print(f'concordance: {score_conc} \\nbrier: {score_brier}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTWs3tqNd-UC",
        "outputId": "e8bb8945-8152-47b8-f397-346d0309ae96"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 0s 4ms/step\n",
            "concordance: 0.6868355646525617 \n",
            "brier: 0.1216432980045617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specific model architectures\n",
        "from pycox.models import LogisticHazard, PMF, DeepHitSingle, MTLR\n",
        "from pycox.evaluation import EvalSurv\n",
        "\n",
        "_ = torch.manual_seed(123)"
      ],
      "metadata": {
        "id": "v4kYThaheRcZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reload as need different parametrization\n",
        "df = pd.read_csv('/content/drive/MyDrive/data_colab/application_training_smart.csv')\n",
        "y_target = np.array(df.tevent.astype(int))\n",
        "y_censoring = np.where(df.event == 1, 1, 0)\n",
        "\n",
        "df_val = pd.read_csv('/content/drive/MyDrive/data_colab/application_test_smart.csv')\n",
        "y_target_val = np.array(df_val.tevent.astype(int))\n",
        "y_censoring_val = np.where(df_val.event == 1, 1, 0)"
      ],
      "metadata": {
        "id": "XL8o3KSTeppd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_periods = y_target.max() + 1\n",
        "\n",
        "features_unscaled = df.loc[:, df.columns[3:]]\n",
        "features_unscaled_val = df_val.loc[:, df_val.columns[3:]]\n",
        "\n",
        "features_numeric = features_unscaled.max(axis=0) > 1\n",
        "\n",
        "numeric_ = features_unscaled.loc[:,features_numeric.values]\n",
        "binary_ = features_unscaled.loc[:,~features_numeric.values].to_numpy()\n",
        "\n",
        "numeric_val = features_unscaled_val.loc[:,features_numeric.values]\n",
        "binary_val = features_unscaled_val.loc[:,~features_numeric.values].to_numpy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(numeric_)\n",
        "\n",
        "numeric_trans = scaler.transform(numeric_)\n",
        "numeric_trans_val = scaler.transform(numeric_val)\n",
        "\n",
        "features = np.concatenate([numeric_trans, binary_], axis=1).astype('float32')\n",
        "features_val = np.concatenate([numeric_trans_val, binary_val], axis=1).astype('float32')"
      ],
      "metadata": {
        "id": "ubx-ZBrBezqf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "_ = torch.manual_seed(123) \n",
        "all_results_smart = {\n",
        "    'LogisticHazard': {},\n",
        "    'PMF': {},\n",
        "    'DeepHitSingle': {},\n",
        "    'MTLR': {},}\n",
        "\n",
        "for mod_ in [LogisticHazard, PMF, DeepHitSingle, MTLR]: \n",
        "    str_pattern = r\"([^\\.]*)$\"\n",
        "    model_key = re.search(str_pattern, str(mod_)).group(0)[:-2]\n",
        "\n",
        "    labtrans = mod_.label_transform(total_periods)\n",
        "    y_train_ = labtrans.fit_transform(*(y_target, y_censoring))\n",
        "\n",
        "    in_features = features.shape[1]\n",
        "    out_features = labtrans.out_features\n",
        "\n",
        "    net = torch.nn.Sequential(\n",
        "        torch.nn.Linear(in_features, 4),\n",
        "        torch.nn.ReLU(),\n",
        "\n",
        "        torch.nn.Linear(4, 16),\n",
        "        torch.nn.ReLU(),\n",
        "\n",
        "        torch.nn.Linear(16, 20),\n",
        "        torch.nn.ReLU(),\n",
        "\n",
        "        torch.nn.Linear(20, 20),\n",
        "        torch.nn.ReLU(),\n",
        "\n",
        "        torch.nn.Linear(20, out_features)\n",
        "    )\n",
        "    \n",
        "    model = mod_(net,\n",
        "                tt.optim.Adam(0.005),\n",
        "                duration_index=labtrans.cuts)\n",
        "\n",
        "    batch_size = 128\n",
        "    epochs = 25\n",
        "\n",
        "    log = model.fit(features, y_train_, batch_size, epochs) \n",
        "    surv = model.predict_surv_df(features_val)\n",
        "    \n",
        "    time_grid = np.linspace(y_target_val.min(), y_target_val.max(), 100)\n",
        "\n",
        "    evaluation_results = EvalSurv(surv,\n",
        "                                    y_target_val,\n",
        "                                    y_censoring_val,\n",
        "                                    censor_surv='km')\n",
        "    concordance_idx = evaluation_results.concordance_td('antolini')\n",
        "    brier_idx = evaluation_results.integrated_brier_score(time_grid) \n",
        "\n",
        "    all_results_smart[model_key]['model'] = model\n",
        "    all_results_smart[model_key]['predictions'] = surv\n",
        "    all_results_smart[model_key]['concordance'] = concordance_idx\n",
        "    all_results_smart[model_key]['brier'] = brier_idx"
      ],
      "metadata": {
        "id": "hmL_vcGDe4Hv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_key in all_results_smart.keys():\n",
        "    print(model_key, np.round(all_results_smart[model_key]['concordance'],3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foEGc1ldfj6g",
        "outputId": "c9eaee9d-ef32-48e7-98c5-9deb1d7a2976"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticHazard 0.63\n",
            "PMF 0.662\n",
            "DeepHitSingle 0.664\n",
            "MTLR 0.677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model_key in all_results_smart.keys():\n",
        "    print(model_key, np.round(all_results_smart[model_key]['brier'],3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0tF48vufr2Y",
        "outputId": "1431bcc2-745d-4add-e301-9787ddffec67"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticHazard 0.112\n",
            "PMF 0.116\n",
            "DeepHitSingle 0.109\n",
            "MTLR 0.112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Economic application "
      ],
      "metadata": {
        "id": "CqJHAGeAgmRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/data_colab/data_econ_train.csv')\n",
        "y_target = np.array(df.time.astype(int))\n",
        "y_censoring = np.where(df.event == 1, False, True)\n",
        "y_censoring_target_eval = np.where(df.event == 1, 1, 0)\n",
        "\n",
        "df_val = pd.read_csv('/content/drive/MyDrive/data_colab/data_econ_test.csv')\n",
        "y_target_val = np.array(df_val.time.astype(int))\n",
        "y_censoring_val = np.where(df_val.event == 1, False, True)\n",
        "y_censoring_val_eval = np.where(df_val.event == 1, 1, 0)"
      ],
      "metadata": {
        "id": "VAQskxwEiS68"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_periods = int(y_target.max())"
      ],
      "metadata": {
        "id": "N0gL3JODkWL-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale features\n",
        "numeric_ = df.loc[:, df.columns[4:-1]]\n",
        "binary_ = np.where(df.fac_ui == 1, 1, 0).reshape(-1,1)\n",
        "\n",
        "numeric_val = df_val.loc[:, df_val.columns[4:-1]]\n",
        "binary_val = np.where(df_val.fac_ui == 1, 1, 0).reshape(-1,1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(numeric_)\n",
        "\n",
        "numeric_trans = scaler.transform(numeric_)\n",
        "numeric_trans_val = scaler.transform(numeric_val)\n",
        "\n",
        "features = np.concatenate([numeric_trans, binary_], axis=1).astype('float32')\n",
        "features_val = np.concatenate([numeric_trans_val, binary_val], axis=1).astype('float32')"
      ],
      "metadata": {
        "id": "xRsqmiZfi51m"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model\n",
        "tf.keras.utils.set_random_seed(123)\n",
        "model_smart = DeepKaplanMeier(total_periods,\n",
        "                              learning_rate=0.05,\n",
        "                              weights_max=1.6)\n",
        "\n",
        "model_smart.compile_model((6, ),\n",
        "                          [14, 12, 8],\n",
        "                          preoutput=20)\n",
        "\n",
        "for repeater_epoch in range(5):\n",
        "  print(f'...this is epoch {5*repeater_epoch+5}')\n",
        "\n",
        "  tf.keras.utils.set_random_seed(123)\n",
        "  model_smart.fit_model(features,\n",
        "                      y_target.astype(int), \n",
        "                      y_censoring,\n",
        "                      epochs=5,\n",
        "                      verbose=False, \n",
        "                      batch_size=256)\n",
        "  \n",
        "preds_raw = model_smart.model.predict(features_val)\n",
        "score_concordance, score_brier = calculate_preds(preds_raw, y_target_val, y_censoring_val_eval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCoEK10y9AQ-",
        "outputId": "c7f8a958-db50-46f2-9fd0-b9bb726277cd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...this is epoch 5\n",
            "...this is epoch 10\n",
            "...this is epoch 15\n",
            "...this is epoch 20\n",
            "...this is epoch 25\n",
            "21/21 [==============================] - 1s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'concordance: {score_concordance} \\nbrier: {score_brier}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avtMRkLjRKox",
        "outputId": "a5fe5ce7-2b99-4b0f-a242-3b4af8d826e3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "concordance: 0.7145001667963972 \n",
            "brier: 0.16716137251218152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "_ = torch.manual_seed(123) \n",
        "all_results_econ = {\n",
        "    'LogisticHazard': {},\n",
        "    'PMF': {},\n",
        "    'DeepHitSingle': {},\n",
        "    'MTLR': {},}\n",
        "\n",
        "for mod_ in [LogisticHazard, PMF, DeepHitSingle, MTLR]: \n",
        "    str_pattern = r\"([^\\.]*)$\"\n",
        "    model_key = re.search(str_pattern, str(mod_)).group(0)[:-2]\n",
        "\n",
        "    labtrans = mod_.label_transform(total_periods)\n",
        "    y_train_ = labtrans.fit_transform(*(y_target, y_censoring_target_eval))\n",
        "\n",
        "    in_features = features.shape[1]\n",
        "    out_features = labtrans.out_features\n",
        "\n",
        "    net = torch.nn.Sequential(\n",
        "        torch.nn.Linear(in_features, 14),\n",
        "        torch.nn.ReLU(),\n",
        "\n",
        "        torch.nn.Linear(14, 12),\n",
        "        torch.nn.ReLU(),\n",
        "\n",
        "        torch.nn.Linear(12, 8),\n",
        "        torch.nn.ReLU(),\n",
        "\n",
        "        torch.nn.Linear(8, 20),\n",
        "        torch.nn.ReLU(),\n",
        "\n",
        "        torch.nn.Linear(20, out_features)\n",
        "    )\n",
        "    \n",
        "    model = mod_(net,\n",
        "                tt.optim.Adam(0.005),\n",
        "                duration_index=labtrans.cuts)\n",
        "\n",
        "    batch_size = 256\n",
        "    epochs = 50\n",
        "\n",
        "    log = model.fit(features, y_train_, batch_size, epochs) \n",
        "    surv = model.predict_surv_df(features_val)\n",
        "    \n",
        "    time_grid = np.linspace(y_target_val.min(), y_target_val.max(), 100)\n",
        "\n",
        "    evaluation_results = EvalSurv(surv,\n",
        "                                    y_target_val,\n",
        "                                    y_censoring_val_eval,\n",
        "                                    censor_surv='km')\n",
        "    concordance_idx = evaluation_results.concordance_td('antolini')\n",
        "    brier_idx = evaluation_results.integrated_brier_score(time_grid) \n",
        "\n",
        "    all_results_econ[model_key]['model'] = model\n",
        "    all_results_econ[model_key]['predictions'] = surv\n",
        "    all_results_econ[model_key]['concordance'] = concordance_idx\n",
        "    all_results_econ[model_key]['brier'] = brier_idx"
      ],
      "metadata": {
        "id": "_BkBW4oIRLUK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_key in all_results_econ.keys():\n",
        "    print(model_key, np.round(all_results_econ[model_key]['concordance'],3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4flNB8KRLV5",
        "outputId": "545eb233-8226-424a-a3d8-d830222e16c1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticHazard 0.708\n",
            "PMF 0.493\n",
            "DeepHitSingle 0.691\n",
            "MTLR 0.544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model_key in all_results_econ.keys():\n",
        "    print(model_key, np.round(all_results_econ[model_key]['brier'],3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxnM5fQ1RLYU",
        "outputId": "c5b48a0f-8b17-43f7-a04f-3588b4065647"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticHazard 0.162\n",
            "PMF 0.167\n",
            "DeepHitSingle 0.179\n",
            "MTLR 0.172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulations (selected)"
      ],
      "metadata": {
        "id": "eEgv2JEyenbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_nonlinear_data(size, seed):\n",
        "    size_ = size\n",
        "    seed_ = seed\n",
        "\n",
        "    np.random.seed(seed_)\n",
        "    x_1 = np.random.exponential(0.1, size=size_)\n",
        "    x_2 = np.random.normal(10, np.sqrt(5), size=size_)\n",
        "    x_3 = np.random.poisson(5, size=size_)\n",
        "\n",
        "    features = np.array([x_1, x_2, x_3]).T\n",
        "\n",
        "    # Arrange Targets\n",
        "    survial_model = np.sin(x_1)*3.14 + 0.318*x_2 + 2.72*np.abs(np.cos(x_3))\n",
        "\n",
        "    true_survival_time = gamma.rvs(survial_model, scale=1, random_state=seed_)\n",
        "    ceiling_surv_ = np.ceil(true_survival_time)\n",
        "\n",
        "    censoring_transformed = np.minimum(2*(survial_model/survial_model.max()),1)\n",
        "\n",
        "    censored_instances = np.random.uniform(size=size_) > (censoring_transformed)\n",
        "    censored_surv = [np.ceil(np.random.uniform()*val_) for val_ in ceiling_surv_[censored_instances]]\n",
        "\n",
        "    observed_survival = ceiling_surv_.copy()\n",
        "    observed_survival[censored_instances] = censored_surv\n",
        "\n",
        "    variable_dict = dict({'true_survival': ceiling_surv_,\n",
        "                        'observed_survival': observed_survival, \n",
        "                        'censoring': censored_instances,\n",
        "                        'features' :  features})\n",
        "\n",
        "    return variable_dict\n",
        "\n",
        "def generate_heavy_censored_data(size, seed):\n",
        "    size_ = size\n",
        "    seed_ = seed\n",
        "\n",
        "    np.random.seed(seed_)\n",
        "    x_1 = np.random.exponential(0.1, size=size_)\n",
        "    x_2 = np.random.normal(10, np.sqrt(5), size=size_)\n",
        "    x_3 = np.random.poisson(5, size=size_)\n",
        "\n",
        "    features = np.array([x_1, x_2, x_3]).T\n",
        "\n",
        "    # Arrange Targets\n",
        "    survial_model = -3.14*x_1 + 0.318*x_2 + 2.72*x_3\n",
        "\n",
        "    true_survival_time = gamma.rvs(survial_model, scale=1, random_state=seed_)\n",
        "    ceiling_surv_ = true_survival_time\n",
        "\n",
        "    censoring_transformed = np.minimum(0.75*(survial_model/survial_model.max()),1)\n",
        "\n",
        "    censored_instances = np.random.uniform(size=size_) > (censoring_transformed)\n",
        "    censored_surv = [np.ceil(np.random.uniform()*val_) for val_ in ceiling_surv_[censored_instances]]\n",
        "\n",
        "    observed_survival = ceiling_surv_.copy()\n",
        "    observed_survival[censored_instances] = censored_surv\n",
        "\n",
        "    variable_dict = dict({'true_survival': ceiling_surv_,\n",
        "                        'observed_survival': observed_survival, \n",
        "                        'censoring': censored_instances,\n",
        "                        'features' :  features})\n",
        "\n",
        "    return variable_dict\n"
      ],
      "metadata": {
        "id": "mWp5jj0qeqMd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nonlinear\n",
        "data_train_nonlin = generate_nonlinear_data(size=3000, seed=42)\n",
        "data_test_nonlin = generate_nonlinear_data(size=1000, seed=123)\n",
        "\n",
        "Nonlinear = namedtuple('Nonlinear', \"train test\")\n",
        "nonlinear_data = Nonlinear(data_train_nonlin, data_test_nonlin)\n",
        "\n",
        "# Heavy censoring\n",
        "data_train_heavy_cens = generate_heavy_censored_data(size=3000, seed=42)\n",
        "data_test_heavy_cens = generate_heavy_censored_data(size=1000, seed=123)\n",
        "data_train_heavy_cens_large = generate_heavy_censored_data(size=12000, seed=42)\n",
        "\n",
        "HeavyCensoring = namedtuple('HeavyCensoring', \"train trainlarge test\")\n",
        "heavy_censored_data = HeavyCensoring(data_train_heavy_cens,\n",
        "                                     data_train_heavy_cens_large,\n",
        "                                     data_test_heavy_cens)"
      ],
      "metadata": {
        "id": "vSEZwOaVfIyP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "_ = torch.manual_seed(123) # still some randomness from the GPU!\n",
        "all_results = {\n",
        "    'LogisticHazard': {},\n",
        "    'PMF': {},\n",
        "    'DeepHitSingle': {},\n",
        "    'MTLR': {},}\n",
        "\n",
        "for dataset_ in [nonlinear_data, heavy_censored_data]:\n",
        "    dataset_key = type(dataset_).__name__\n",
        "    print(dataset_key)\n",
        "    \n",
        "    scaler_ = StandardScaler()\n",
        "    scaler_.fit(dataset_.train['features'])\n",
        "\n",
        "    scaled_features_train = scaler_.transform(dataset_.train['features']).astype('float32')\n",
        "    scaled_features_test = scaler_.transform(dataset_.test['features']).astype('float32')\n",
        "    total_periods = int(dataset_.train['observed_survival'].max()) + 1\n",
        "\n",
        "    censoring_numeric = ~dataset_.train['censoring']*1\n",
        "    survial_numeric = dataset_.train['observed_survival']\n",
        "\n",
        "    censoring_numeric_test = ~dataset_.test['censoring']*1\n",
        "    survial_numeric_test = dataset_.test['observed_survival']\n",
        "    time_grid = np.linspace(survial_numeric_test.min(), survial_numeric_test.max(), 50)\n",
        "\n",
        "\n",
        "    for mod_ in [LogisticHazard, PMF, DeepHitSingle, MTLR]: \n",
        "        str_pattern = r\"([^\\.]*)$\"\n",
        "        model_key = re.search(str_pattern, str(mod_)).group(0)[:-2]\n",
        "\n",
        "        labtrans = mod_.label_transform(total_periods)\n",
        "        y_train_ = labtrans.fit_transform(*(survial_numeric, censoring_numeric))\n",
        "\n",
        "        in_features = scaled_features_train.shape[1]\n",
        "        out_features = labtrans.out_features\n",
        "\n",
        "        net = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features, 20),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            torch.nn.Linear(20, 20),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            torch.nn.Linear(20, 12),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            torch.nn.Linear(12, out_features)\n",
        "        )\n",
        "        \n",
        "        model = mod_(net,\n",
        "                     tt.optim.Adam(0.01),\n",
        "                     duration_index=labtrans.cuts)\n",
        "\n",
        "        batch_size = 128\n",
        "        epochs = 100\n",
        "        _ = torch.manual_seed(123)\n",
        "        log = model.fit(scaled_features_train, y_train_, batch_size, epochs) \n",
        "        surv = model.predict_surv_df(scaled_features_test)\n",
        "\n",
        "        evaluation_results = EvalSurv(surv,\n",
        "                                      survial_numeric_test,\n",
        "                                      censoring_numeric_test,\n",
        "                                      censor_surv='km')\n",
        "        concordance_idx = evaluation_results.concordance_td('antolini')\n",
        "        brier_idx = evaluation_results.integrated_brier_score(time_grid) \n",
        "\n",
        "        all_results[model_key][dataset_key] = {}\n",
        "        all_results[model_key][dataset_key]['model'] = model\n",
        "        all_results[model_key][dataset_key]['predictions'] = surv\n",
        "        all_results[model_key][dataset_key]['concordance'] = concordance_idx\n",
        "        all_results[model_key][dataset_key]['brier'] = brier_idx"
      ],
      "metadata": {
        "id": "ZrgiFdvcfUVd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_type in  all_results.keys():\n",
        "    for data_type in all_results[model_type]:\n",
        "        print(model_type, data_type, np.round(all_results[model_type][data_type]['brier'],3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-qkKwnEnIvv",
        "outputId": "2216b449-9f03-4ee1-ecb0-404574c542d4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticHazard Nonlinear 0.094\n",
            "LogisticHazard HeavyCensoring 0.06\n",
            "PMF Nonlinear 0.092\n",
            "PMF HeavyCensoring 0.06\n",
            "DeepHitSingle Nonlinear 0.095\n",
            "DeepHitSingle HeavyCensoring 0.074\n",
            "MTLR Nonlinear 0.091\n",
            "MTLR HeavyCensoring 0.061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model_type in  all_results.keys():\n",
        "    for data_type in all_results[model_type]:\n",
        "        print(model_type, data_type, np.round(all_results[model_type][data_type]['concordance'],3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_aAms_OffFl",
        "outputId": "58503d55-d006-4803-d7f0-1ba563c56a64"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticHazard Nonlinear 0.61\n",
            "LogisticHazard HeavyCensoring 0.803\n",
            "PMF Nonlinear 0.632\n",
            "PMF HeavyCensoring 0.806\n",
            "DeepHitSingle Nonlinear 0.628\n",
            "DeepHitSingle HeavyCensoring 0.799\n",
            "MTLR Nonlinear 0.63\n",
            "MTLR HeavyCensoring 0.776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Set up model, use max observed survival for periods, run a sample model\n",
        "total_periods = int(nonlinear_data.train['observed_survival'].max())\n",
        "\n",
        "tf.keras.utils.set_random_seed(123)\n",
        "deep_model_simple = DeepKaplanMeier(total_periods)\n",
        "deep_model_simple.compile_model((3, ), [20,20], preoutput=12)"
      ],
      "metadata": {
        "id": "YJ0_M1K7gp-d"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_ = nonlinear_data\n",
        "\n",
        "scaler_ = StandardScaler()\n",
        "scaler_.fit(dataset_.train['features'])\n",
        "\n",
        "scaled_features_train = scaler_.transform(dataset_.train['features']).astype('float32')\n",
        "scaled_features_test = scaler_.transform(dataset_.test['features']).astype('float32')\n",
        "total_periods = int(dataset_.train['observed_survival'].max()) + 1\n",
        "\n",
        "censoring_numeric = dataset_.train['censoring']\n",
        "survial_numeric = dataset_.train['observed_survival']\n",
        "\n",
        "censoring_numeric_test = ~dataset_.test['censoring']*1\n",
        "survial_numeric_test = dataset_.test['observed_survival']\n",
        "time_grid = np.linspace(survial_numeric_test.min(), survial_numeric_test.max(), 50)"
      ],
      "metadata": {
        "id": "AEDvIb2mjtJv"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model\n",
        "tf.keras.utils.set_random_seed(123)\n",
        "for repeater_epoch in range(10):\n",
        "  print(f'...this is epoch {10*repeater_epoch+5}')\n",
        "  deep_model_simple.fit_model(scaled_features_train,\n",
        "                              survial_numeric.astype(int), \n",
        "                              censoring_numeric,\n",
        "                              epochs=10, \n",
        "                              verbose=False, \n",
        "                              batch_size=128\n",
        "                              )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bqjG5FNgvEF",
        "outputId": "d15d90c6-d508-430d-8c6a-6fb567720c08"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...this is epoch 5\n",
            "...this is epoch 15\n",
            "...this is epoch 25\n",
            "...this is epoch 35\n",
            "...this is epoch 45\n",
            "...this is epoch 55\n",
            "...this is epoch 65\n",
            "...this is epoch 75\n",
            "...this is epoch 85\n",
            "...this is epoch 95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_raw = deep_model_simple.model.predict(scaled_features_test)\n",
        "score_concordance, score_brier = calculate_preds(preds_raw,\n",
        "                                                 survial_numeric_test,\n",
        "                                                 censoring_numeric_test)\n",
        "  \n",
        "print(score_concordance, score_brier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQkAoMxOk4uk",
        "outputId": "b10d28b1-9437-4cc1-af99-5fd30f20d6f4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 1s 3ms/step\n",
            "0.6377901025921783 0.09087990661832093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Set up model, use max observed survival for periods, run a sample model\n",
        "total_periods = int(heavy_censored_data.train['observed_survival'].max())\n",
        "\n",
        "tf.keras.utils.set_random_seed(123)\n",
        "deep_model_simple = DeepKaplanMeier(total_periods)\n",
        "deep_model_simple.compile_model((3, ), [20,20,20], preoutput=12)"
      ],
      "metadata": {
        "id": "HI5uxjDtk_zl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_ = heavy_censored_data\n",
        "\n",
        "scaler_ = StandardScaler()\n",
        "scaler_.fit(dataset_.train['features'])\n",
        "\n",
        "scaled_features_train = scaler_.transform(dataset_.train['features']).astype('float32')\n",
        "scaled_features_test = scaler_.transform(dataset_.test['features']).astype('float32')\n",
        "total_periods = int(dataset_.train['observed_survival'].max()) + 1\n",
        "\n",
        "censoring_numeric = dataset_.train['censoring']\n",
        "survial_numeric = dataset_.train['observed_survival']\n",
        "\n",
        "censoring_numeric_test = ~dataset_.test['censoring']*1\n",
        "survial_numeric_test = dataset_.test['observed_survival']\n",
        "time_grid = np.linspace(survial_numeric_test.min(), survial_numeric_test.max(), 50)"
      ],
      "metadata": {
        "id": "mp-7bNGSk_e9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model\n",
        "tf.keras.utils.set_random_seed(123)\n",
        "for repeater_epoch in range(10):\n",
        "  print(f'...this is epoch {10*repeater_epoch+5}')\n",
        "  deep_model_simple.fit_model(scaled_features_train,\n",
        "                              survial_numeric.astype(int), \n",
        "                              censoring_numeric,\n",
        "                              epochs=10, \n",
        "                              verbose=False, \n",
        "                              batch_size=128\n",
        "                              )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR1AtRI8k-_u",
        "outputId": "54fcbbf3-025e-4200-9669-8bf552ae69cb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...this is epoch 5\n",
            "...this is epoch 15\n",
            "...this is epoch 25\n",
            "...this is epoch 35\n",
            "...this is epoch 45\n",
            "...this is epoch 55\n",
            "...this is epoch 65\n",
            "...this is epoch 75\n",
            "...this is epoch 85\n",
            "...this is epoch 95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_raw = deep_model_simple.model.predict(scaled_features_test)\n",
        "score_concordance, score_brier = calculate_preds(preds_raw,\n",
        "                                                 survial_numeric_test,\n",
        "                                                 censoring_numeric_test)\n",
        "  \n",
        "print(score_concordance, score_brier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja-Z-5U3l2nv",
        "outputId": "2ad927f7-5af6-4b8d-a683-ef22415132a4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 1s 5ms/step\n",
            "0.8199628441141699 0.05973803489967512\n"
          ]
        }
      ]
    }
  ]
}