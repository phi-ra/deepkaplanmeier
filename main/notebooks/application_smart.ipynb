{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Kaplan Meier Estimation\n",
    "\n",
    "Before running the notebook make sure that:\n",
    "\n",
    "1. Created a python/conda environment according to the specifications\n",
    "2. Set your working directory (`project_dir`) to the directory where you unpacked the .zip file\n",
    "3. You might want to add a shebang line to the ./utils (however using it from notebooks with a specified environment should work as well)\n",
    "\n",
    "The notebook genereates data, trains models and saves them into the ./data directory. Note that at times you will need to overwrite existing files (this is to ensure that you do not accidentally overwrite files that took a long time training - aka. model weights)\n",
    "\n",
    "See R-Scrips for visualisations (by default, all visualisations should already be in the zip folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping\n",
    "import os\n",
    "os.chdir(project_dir)\n",
    "\n",
    "# Wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product as itp\n",
    "import pickle\n",
    "\n",
    "# Stats\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# TF & Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from main.utils.conditional_km import DeepKaplanMeier\n",
    "from main.utils.metrics import calculate_concordance_surv\n",
    "train_ = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/application/application_training_smart.csv')\n",
    "y_target = df.tevent.astype(int)\n",
    "y_censoring = np.where(df.event == 1, False, True)\n",
    "\n",
    "df_val = pd.read_csv('data/application/application_test_smart.csv')\n",
    "y_target_val = df_val.tevent.astype(int)\n",
    "y_censoring_val = np.where(df_val.event == 1, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_unscaled = df.loc[:, df.columns[3:]]\n",
    "features_unscaled_val = df_val.loc[:, df_val.columns[3:]]\n",
    "\n",
    "features_numeric = features_unscaled.max(axis=0) > 1\n",
    "\n",
    "numeric_ = features_unscaled.loc[:,features_numeric.values]\n",
    "binary_ = features_unscaled.loc[:,~features_numeric.values].to_numpy()\n",
    "\n",
    "numeric_val = features_unscaled_val.loc[:,features_numeric.values]\n",
    "binary_val = features_unscaled_val.loc[:,~features_numeric.values].to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(numeric_)\n",
    "\n",
    "numeric_trans = scaler.transform(numeric_)\n",
    "numeric_trans_val = scaler.transform(numeric_val)\n",
    "\n",
    "features = np.concatenate([numeric_trans, binary_], axis=1)\n",
    "features_val = np.concatenate([numeric_trans_val, binary_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_periods = df.tevent.max()\n",
    "input_dim = (features.shape[1], )\n",
    "input_shape = input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_mod = DeepKaplanMeier(int(total_periods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 18:01:53.039306: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-10-20 18:01:53.039407: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "y_matrix, weight_matrix = init_mod.prepare_survival(list(y_target), list(y_censoring))\n",
    "y_matrix_val, weight_matrix_val = init_mod.prepare_survival(list(y_target_val), list(y_censoring_val))\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(tuple([features] + [y_matrix.reshape(-1,36)] + [weight_matrix.reshape(-1,36)] + [y_target]))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(tuple([features_val] + [y_matrix_val.reshape(-1,36)] + [weight_matrix_val.reshape(-1,36)] + [y_target_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_:\n",
    "    input_shape = input_dim\n",
    "\n",
    "    total_trials = 150\n",
    "\n",
    "    # Optimize layer 1-3 plus additional layer\n",
    "    # Optimize learning rate\n",
    "    # Optimize batchsize\n",
    "    layer_sizes = np.arange(3,13, 3)\n",
    "    learning_rate = [5e-02, 1e-02, 5e-03, 1e-03]\n",
    "    batch_sizes = [32,64,128,256]\n",
    "\n",
    "    np.random.seed(123)\n",
    "    for trial_ in range(total_trials):\n",
    "        trial_dict[f\"trial_{trial_}\"] = {}\n",
    "\n",
    "        check_indices = np.ceil(np.random.uniform(size=6)*4)-1\n",
    "        check_indices = check_indices.astype(int)\n",
    "\n",
    "        hp_unit_1 = layer_sizes[check_indices[0]]\n",
    "        hp_unit_2 = layer_sizes[check_indices[1]]\n",
    "        hp_unit_3 = layer_sizes[check_indices[2]]\n",
    "        hp_unit_4 = layer_sizes[check_indices[3]]\n",
    "\n",
    "        hp_learning_rate = learning_rate[check_indices[4]]\n",
    "        hp_batch = batch_sizes[check_indices[5]]\n",
    "\n",
    "        trial_dict[f\"trial_{trial_}\"]['hp_unit_1'] = hp_unit_1\n",
    "        trial_dict[f\"trial_{trial_}\"]['hp_unit_2'] = hp_unit_2\n",
    "        trial_dict[f\"trial_{trial_}\"]['hp_unit_3'] = hp_unit_3\n",
    "        trial_dict[f\"trial_{trial_}\"]['hp_unit_4'] = hp_unit_4\n",
    "        trial_dict[f\"trial_{trial_}\"]['hp_learning_rate'] = hp_learning_rate\n",
    "        trial_dict[f\"trial_{trial_}\"]['hp_batch'] = hp_batch\n",
    "        trial_dict[f\"trial_{trial_}\"]['epochs'] = {}\n",
    "\n",
    "        print(trial_dict[f\"trial_{trial_}\"])\n",
    "\n",
    "        model_raw = init_mod.return_comp_graph_raw(input_shape=input_shape, \n",
    "                                                hidden_units=[hp_unit_1, hp_unit_2, hp_unit_3],\n",
    "                                                preoutput=hp_unit_4)\n",
    "\n",
    "        upper_bound = np.random.uniform(1,3.5)\n",
    "        trial_dict[f\"trial_{trial_}\"]['weight'] = upper_bound\n",
    "        weight_decrease = 1 + np.linspace(0,upper_bound,36)\n",
    "        weight_array = weight_decrease\n",
    "        best_epoch_loss = 0\n",
    "\n",
    "        #### Update variables ####\n",
    "        \n",
    "        batch_size=hp_batch\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
    "        loss_objective = keras.losses.BinaryCrossentropy()\n",
    "        epoch_loss_metric = keras.metrics.Mean()\n",
    "\n",
    "        train_ds = train_dataset.batch(batch_size)\n",
    "        check_dataset = train_dataset.shuffle(10)\n",
    "        check_dataset = check_dataset.take(775)\n",
    "        check_ds = check_dataset.batch(775)\n",
    "\n",
    "        for epoch in range(75):\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            if best_epoch_loss < 0.5 and epoch > 10:\n",
    "                continue\n",
    "            \n",
    "            for step, (features_, _labels, _weights, _) in enumerate(train_ds):\n",
    "                with tf.GradientTape() as tape:\n",
    "                        preds_ = model_raw(features_, training=True)\n",
    "                        losses = [loss_objective(_labels[:,i], preds_[i], sample_weight=tf.reshape(_weights[:,i], (-1,1))) for i in range(len(preds_))]\n",
    "                        losses = [tf.multiply(loss_, weight_) for loss_, weight_ in zip(losses, weight_array)]\n",
    "                        gradients = tape.gradient(losses, model_raw.trainable_variables)\n",
    "                        optimizer.apply_gradients(zip(gradients, model_raw.trainable_variables))\n",
    "\n",
    "            for features_, _labels, _weights, true_ in check_ds:\n",
    "                preds_test = model_raw(features_)\n",
    "                preds_test = pd.DataFrame(list(map(np.ravel, preds_test)))\n",
    "\n",
    "                nom_, denom_, val_ = calculate_concordance_surv(y_target_val, \n",
    "                                                                y_censoring_val, \n",
    "                                                                preds_test)\n",
    "\n",
    "                \n",
    "                epoch_loss_metric = val_\n",
    "                \n",
    "            trial_dict[f\"trial_{trial_}\"]['epochs'][f'epoch_{epoch}'] = epoch_loss_metric\n",
    "            best_epoch_loss = max(best_epoch_loss, epoch_loss_metric)\n",
    "            print(f'best validation result is: {best_epoch_loss}')\n",
    "            epoch_loss_metric = 0\n",
    "\n",
    "\n",
    "        trial_dict[f\"trial_{trial_}\"]['best_validation_result'] = best_epoch_loss\n",
    "        with open('./model_tuning/manual_gridsearch/tuning_smart_data.pkl', 'wb') as con:\n",
    "            pickle.dump(trial_dict, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "if train_:\n",
    "        \n",
    "    hp_unit_1  = 4\n",
    "    hp_unit_2  = 12\n",
    "    hp_unit_3  = 12\n",
    "    hp_unit_4  = 12\n",
    "    hp_batch =  256\n",
    "    hp_learning_rate = 0.005\n",
    "    hp_weight = 0\n",
    "    hp_epochs = 30\n",
    "\n",
    "    model_raw = init_mod.return_comp_graph_raw(input_shape=input_shape, \n",
    "                                            hidden_units=[hp_unit_1, hp_unit_2, hp_unit_3],\n",
    "                                            preoutput=hp_unit_4)\n",
    "\n",
    "    batch_size=hp_batch\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate)\n",
    "    loss_objective = keras.losses.BinaryCrossentropy()\n",
    "    train_ds = train_dataset.batch(batch_size)\n",
    "\n",
    "    weight_decrease = 1 + np.linspace(0,hp_weight,36)\n",
    "    weight_array = weight_decrease\n",
    "\n",
    "    for epoch in range(hp_epochs):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        for step, (features_, _labels, _weights, _) in enumerate(train_ds):\n",
    "            with tf.GradientTape() as tape:\n",
    "                    preds_ = model_raw(features_, training=True)\n",
    "                    losses = [loss_objective(_labels[:,i], preds_[i], sample_weight=tf.reshape(_weights[:,i], (-1,1))) for i in range(len(preds_))]\n",
    "                    losses = [tf.multiply(loss_, weight_) for loss_, weight_ in zip(losses, weight_array)]\n",
    "                    gradients = tape.gradient(losses, model_raw.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model_raw.trainable_variables))\n",
    "\n",
    "    model_smart = model_raw\n",
    "else:\n",
    "    model_smart = tf.keras.models.load_model('data/model_weights/model_smart_application.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-index is: 0.693\n"
     ]
    }
   ],
   "source": [
    "preds_test = model_smart(features_val)\n",
    "preds_test = pd.DataFrame(list(map(np.ravel, preds_test)))\n",
    "\n",
    "nom_, denom_, val_ = calculate_concordance_surv(y_target_val, \n",
    "                                        y_censoring_val, \n",
    "                                        preds_test)\n",
    "print(f\"C-index is: {np.round(val_, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test.to_csv('data/application/results_smart_ckm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cox PH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cox = df.drop(columns=['rowid'])\n",
    "df_cox_test = df_val.drop(columns=['event'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph = CoxPHFitter()\n",
    "cph.fit(df_cox, duration_col='tevent', event_col='event')\n",
    "\n",
    "preds_cox = cph.predict_survival_function(df_cox_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cox.to_csv('data/application/results_smart_cox.csv', \n",
    "index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance is: 0.669\n"
     ]
    }
   ],
   "source": [
    "preds_cox = cph.predict_survival_function(df_cox_test)\n",
    "\n",
    "nom_, denom_, val_ = calculate_concordance_surv(y_target_val, \n",
    "                                                y_censoring_val, \n",
    "                                                preds_cox)\n",
    "print(f\"Concordance is: {np.round(val_,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaplan_meier = KaplanMeierFitter()\n",
    "kaplan_meier.fit(y_target_val,\n",
    "                 ~y_censoring_val)\n",
    "\n",
    "kaplan_meier_survival = kaplan_meier.survival_function_\n",
    "\n",
    "# Export for plotting\n",
    "(kaplan_meier_survival.reset_index()\n",
    "                      .to_csv('data/application/results_smart_km.csv', \n",
    "                      index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tens_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:21:17) \n[Clang 11.1.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbbf41762ef44221e9bab1cb88f3143b98cae4bbdab9afd61221d8b0b6f6be23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
