{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Kaplan Meier Estimation\n",
    "\n",
    "Before running the notebook make sure that:\n",
    "\n",
    "1. Created a python/conda environment according to the specifications\n",
    "2. Set your working directory (`project_dir`) to the directory where you unpacked the .zip file\n",
    "3. You might want to add a shebang line to the ./utils (however using it from notebooks with a specified environment should work as well)\n",
    "\n",
    "The notebook genereates data, trains models and saves them into the ./data directory. Note that at times you will need to overwrite existing files (this is to ensure that you do not accidentally overwrite files that took a long time training - aka. model weights)\n",
    "\n",
    "See R-Scrips for visualisations (by default, all visualisations should already be in the zip folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = '/Users/philippratz/Documents/Uni/PhD/UQAM/research/joint_estimation/cond_km/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping\n",
    "import os\n",
    "os.chdir(project_dir)\n",
    "\n",
    "# Wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product as itp\n",
    "import pickle\n",
    "\n",
    "# Stats\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# TF & Keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from main.utils.conditional_km import DeepKaplanMeier\n",
    "from main.utils.metrics import calculate_concordance_surv\n",
    "train_ = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../kaplan_meier/data/application/data_econ_train.csv')\n",
    "y_target = df.time.astype(int)\n",
    "y_censoring = np.where(df.event == 1, False, True)\n",
    "\n",
    "df_val = pd.read_csv('../kaplan_meier/data/application/data_econ_test.csv')\n",
    "y_target_val = df_val.time.astype(int)\n",
    "y_censoring_val = np.where(df_val.event == 1, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/application/data_econ_train.csv')\n",
    "y_target = df.time.astype(int)\n",
    "y_censoring = np.where(df.event == 1, False, True)\n",
    "\n",
    "df_val = pd.read_csv('data/application/data_econ_test.csv')\n",
    "y_target_val = df_val.time.astype(int)\n",
    "y_censoring_val = np.where(df_val.event == 1, False, True)\n",
    "\n",
    "df_example = pd.read_csv('data/application/example_econ.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "\n",
    "numeric_ = df.loc[:, df.columns[4:-1]]\n",
    "binary_ = np.where(df.fac_ui == 'yes', 1, 0).reshape(-1,1)\n",
    "\n",
    "numeric_val = df_val.loc[:, df_val.columns[4:-1]]\n",
    "binary_val = np.where(df_val.fac_ui == 'yes', 1, 0).reshape(-1,1)\n",
    "\n",
    "numeric_example = df_example.loc[:, df_val.columns[4:-1]]\n",
    "binary_example = np.where(df_example.fac_ui == 'yes', 1, 0).reshape(-1,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(numeric_)\n",
    "\n",
    "numeric_trans = scaler.transform(numeric_)\n",
    "numeric_trans_val = scaler.transform(numeric_val)\n",
    "numeric_trans_example = scaler.transform(numeric_example)\n",
    "\n",
    "features = np.concatenate([numeric_trans, binary_], axis=1)\n",
    "features_val = np.concatenate([numeric_trans_val, binary_val], axis=1)\n",
    "features_example = np.concatenate([numeric_trans_example, binary_example], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model inputs\n",
    "total_periods = df.time.max()\n",
    "input_dim = (features.shape[1], )\n",
    "input_shape = input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_mod = DeepKaplanMeier(int(total_periods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 18:40:42.418433: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-10-20 18:40:42.418547: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "y_matrix, weight_matrix = init_mod.prepare_survival(list(y_target), list(y_censoring))\n",
    "y_matrix_val, weight_matrix_val = init_mod.prepare_survival(list(y_target_val), list(y_censoring_val))\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(tuple([features] + [y_matrix.reshape(-1,total_periods+1)] + [weight_matrix.reshape(-1,total_periods+1)] + [y_target]))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(tuple([features_val] + [y_matrix_val.reshape(-1,total_periods+1)] + [weight_matrix_val.reshape(-1,total_periods+1)] + [y_target_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_:\n",
    "    input_shape = input_dim\n",
    "\n",
    "    total_trials = 150\n",
    "\n",
    "    # Optimize layer 1-3 plus additional layer\n",
    "    # Optimize learning rate\n",
    "    # Optimize batchsize\n",
    "\n",
    "    layer_sizes = np.arange(3,13, 3)\n",
    "    learning_rate = [5e-02, 1e-02, 5e-03, 1e-03]\n",
    "    batch_sizes = [32,64,128,256]\n",
    "\n",
    "    np.random.seed(123)\n",
    "    for trial_ in range(total_trials):\n",
    "        trial_dict[f\"trial_{trial_}\"] = {}\n",
    "\n",
    "        check_indices = np.ceil(np.random.uniform(size=6)*4)-1\n",
    "        check_indices = check_indices.astype(int)\n",
    "\n",
    "        hp_unit_1 = layer_sizes[check_indices[0]]\n",
    "        hp_unit_2 = layer_sizes[check_indices[1]]\n",
    "        hp_unit_3 = layer_sizes[check_indices[2]]\n",
    "        hp_unit_4 = layer_sizes[check_indices[3]]\n",
    "\n",
    "        hp_learning_rate = learning_rate[check_indices[4]]\n",
    "        hp_batch = batch_sizes[check_indices[5]]\n",
    "\n",
    "        trial_dict[f\"trial_{trial_}\"]['hp_unit_1'] = hp_unit_1\n",
    "        trial_dict[f\"trial_{trial_}\"]['hp_unit_2'] = hp_unit_2\n",
    "        trial_dict[f\"trial_{trial_}\"]['hp_unit_3'] = hp_unit_3\n",
    "        trial_dict[f\"trial_{trial_}\"]['hp_unit_4'] = hp_unit_4\n",
    "        trial_dict[f\"trial_{trial_}\"]['hp_learning_rate'] = hp_learning_rate\n",
    "        trial_dict[f\"trial_{trial_}\"]['hp_batch'] = hp_batch\n",
    "        trial_dict[f\"trial_{trial_}\"]['epochs'] = {}\n",
    "\n",
    "        print(trial_dict[f\"trial_{trial_}\"])\n",
    "\n",
    "        model_raw = init_mod.return_comp_graph_raw(input_shape=input_shape, \n",
    "                                                hidden_units=[hp_unit_1, hp_unit_2, hp_unit_3],\n",
    "                                                preoutput=hp_unit_4)\n",
    "\n",
    "        upper_bound = np.random.uniform(1,2.5)\n",
    "        trial_dict[f\"trial_{trial_}\"]['weight'] = upper_bound\n",
    "        weight_decrease = 1 + np.linspace(0,upper_bound,36)\n",
    "        weight_array = weight_decrease\n",
    "\n",
    "        #### Update variables ####\n",
    "        \n",
    "        batch_size=hp_batch\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
    "        loss_objective = keras.losses.BinaryCrossentropy()\n",
    "        epoch_loss_metric = keras.metrics.Mean()\n",
    "\n",
    "        train_ds = train_dataset.batch(batch_size)\n",
    "        check_dataset = train_dataset.shuffle(10)\n",
    "        check_dataset = check_dataset.take(648)\n",
    "        check_ds = check_dataset.batch(648)\n",
    "\n",
    "        best_epoch_loss = 0\n",
    "        epoch_checker = 0\n",
    "        val_ = 0\n",
    "\n",
    "        for epoch in range(75):\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            if best_epoch_loss > val_:\n",
    "                epoch_checker += 1\n",
    "            else:\n",
    "                epoch_checker = 0\n",
    "\n",
    "            if epoch_checker > 10:\n",
    "                continue\n",
    "            \n",
    "            for step, (features_, _labels, _weights, _) in enumerate(train_ds):\n",
    "                with tf.GradientTape() as tape:\n",
    "                        preds_ = model_raw(features_, training=True)\n",
    "                        losses = [loss_objective(_labels[:,i], preds_[i], sample_weight=tf.reshape(_weights[:,i], (-1,1))) for i in range(len(preds_))]\n",
    "                        losses = [tf.multiply(loss_, weight_) for loss_, weight_ in zip(losses, weight_array)]\n",
    "                        gradients = tape.gradient(losses, model_raw.trainable_variables)\n",
    "                        optimizer.apply_gradients(zip(gradients, model_raw.trainable_variables))\n",
    "\n",
    "            for features_, _labels, _weights, true_ in check_ds:\n",
    "                preds_test = model_raw(features_)\n",
    "                preds_test = pd.DataFrame(list(map(np.ravel, preds_test)))\n",
    "\n",
    "                nom_, denom_, val_ = calculate_concordance_surv(y_target_val, \n",
    "                                                                y_censoring_val, \n",
    "                                                                preds_test)\n",
    "\n",
    "                \n",
    "                epoch_loss_metric = val_\n",
    "                \n",
    "            trial_dict[f\"trial_{trial_}\"]['epochs'][f'epoch_{epoch}'] = epoch_loss_metric\n",
    "            best_epoch_loss = max(best_epoch_loss, epoch_loss_metric)\n",
    "            print(f'best validation result is: {best_epoch_loss}')\n",
    "            if best_epoch_loss > 0.705 and best_epoch_loss == epoch_loss_metric:\n",
    "                model_raw.save_weights(f'./data/models/check_unemp_{trial_}')\n",
    "\n",
    "            epoch_loss_metric = 0\n",
    "\n",
    "\n",
    "        trial_dict[f\"trial_{trial_}\"]['best_validation_result'] = best_epoch_loss\n",
    "        with open('./model_tuning/manual_gridsearch/tuning_unemployment_data.pkl', 'wb') as con:\n",
    "            pickle.dump(trial_dict, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_unit_1  = 12\n",
    "hp_unit_2  = 6\n",
    "hp_unit_3  = 12\n",
    "hp_unit_4  = 9\n",
    "hp_batch =  128\n",
    "hp_learning_rate = 0.005\n",
    "hp_epoch = 20\n",
    "hp_weight = 0.15\n",
    "\n",
    "model_raw = init_mod.return_comp_graph_raw(input_shape=input_shape, \n",
    "                                        hidden_units=[hp_unit_1, hp_unit_2, hp_unit_3],\n",
    "                                        preoutput=hp_unit_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x105be7040>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_raw.load_weights('../kaplan_meier/data/models/model_weights_unemployment_applications')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "if train_:\n",
    "    tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "    hp_unit_1  = 12\n",
    "    hp_unit_2  = 6\n",
    "    hp_unit_3  = 12\n",
    "    hp_unit_4  = 9\n",
    "    hp_batch =  128\n",
    "    hp_learning_rate = 0.005\n",
    "    hp_epoch = 20\n",
    "    hp_weight = 0.15\n",
    "\n",
    "    model_raw = init_mod.return_comp_graph_raw(input_shape=input_shape, \n",
    "                                            hidden_units=[hp_unit_1, hp_unit_2, hp_unit_3],\n",
    "                                            preoutput=hp_unit_4)\n",
    "\n",
    "    weight_decrease = 1 + np.linspace(0,hp_weight,36)\n",
    "    weight_array = weight_decrease\n",
    "\n",
    "    batch_size=hp_batch\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
    "    loss_objective = tf.keras.losses.BinaryCrossentropy()\n",
    "    epoch_loss_metric = tf.keras.metrics.Mean()\n",
    "    train_ds = train_dataset.batch(batch_size)\n",
    "\n",
    "    for epoch in range(hp_epoch):\n",
    "        for step, (features_, _labels, _weights, _) in enumerate(train_ds):\n",
    "            with tf.GradientTape() as tape:\n",
    "                    preds_ = model_raw(features_, training=True)\n",
    "                    losses = [loss_objective(_labels[:,i], preds_[i], sample_weight=tf.reshape(_weights[:,i], (-1,1))) for i in range(len(preds_))]\n",
    "                    losses = [tf.multiply(loss_, weight_) for loss_, weight_ in zip(losses, weight_array)]\n",
    "                    gradients = tape.gradient(losses, model_raw.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model_raw.trainable_variables))\n",
    "\n",
    "    model_econ = model_raw\n",
    "\n",
    "else:\n",
    "    model_econ = tf.keras.models.load_model('data/model_weights/model_unemp_application.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4796301562648116\n",
      "1 0.4809864444104157\n",
      "2 0.47586025856977227\n",
      "3 0.4778801070446773\n",
      "4 0.48280941234805563\n",
      "5 0.488300191776227\n",
      "6 0.49301803279883916\n",
      "7 0.49834839104849826\n",
      "8 0.5027089303553329\n",
      "9 0.5071569721231743\n",
      "10 0.5112914634057416\n",
      "11 0.5151634473052888\n",
      "12 0.5187145888478113\n",
      "13 0.5217480074960441\n",
      "14 0.5237751478426997\n",
      "15 0.5253501921408207\n",
      "16 0.5274356674614807\n",
      "17 0.5290909223488577\n",
      "18 0.5300169900611787\n",
      "19 0.5308993065429966\n",
      "20 0.5321316328688411\n",
      "21 0.5330358249659105\n",
      "22 0.5335170885014474\n",
      "23 0.534005643908735\n",
      "24 0.53407127075449\n",
      "25 0.5343410700092607\n",
      "26 0.5346619123662854\n",
      "27 0.5349462953645572\n",
      "28 0.5352015108758267\n",
      "29 0.5351723433888245\n",
      "30 0.5352817214650829\n",
      "31 0.5359452817943838\n",
      "32 0.5361057029728961\n",
      "33 0.5359234061791321\n",
      "34 0.5361713298186511\n",
      "35 0.5365286315344285\n",
      "36 0.5370463544287183\n",
      "37 0.5374765748620013\n",
      "38 0.5375786610665092\n",
      "39 0.5377463741167721\n"
     ]
    }
   ],
   "source": [
    "hp_unit_1  = 12\n",
    "hp_unit_2  = 6\n",
    "hp_unit_3  = 12\n",
    "hp_unit_4  = 9\n",
    "hp_batch =  128\n",
    "hp_learning_rate = 0.001\n",
    "hp_epoch = 40\n",
    "hp_weight = 1.5\n",
    "\n",
    "model_raw = init_mod.return_comp_graph_raw(input_shape=input_shape, \n",
    "                                        hidden_units=[hp_unit_1, hp_unit_2, hp_unit_3],\n",
    "                                        preoutput=hp_unit_4)\n",
    "\n",
    "weight_decrease = 1 + np.linspace(0,hp_weight,36)\n",
    "weight_array = weight_decrease\n",
    "\n",
    "batch_size=hp_batch\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
    "loss_objective = tf.keras.losses.BinaryCrossentropy()\n",
    "epoch_loss_metric = tf.keras.metrics.Mean()\n",
    "train_ds = train_dataset.batch(batch_size)\n",
    "\n",
    "for epoch in range(hp_epoch):\n",
    "    for step, (features_, _labels, _weights, _) in enumerate(train_ds):\n",
    "        with tf.GradientTape() as tape:\n",
    "                preds_ = model_raw(features_, training=True)\n",
    "                losses = [loss_objective(_labels[:,i], preds_[i], sample_weight=tf.reshape(_weights[:,i], (-1,1))) for i in range(len(preds_))]\n",
    "                losses = [tf.multiply(loss_, weight_) for loss_, weight_ in zip(losses, weight_array)]\n",
    "                gradients = tape.gradient(losses, model_raw.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model_raw.trainable_variables))\n",
    "\n",
    "    preds_test = model_raw(features_val)\n",
    "    preds_test = pd.DataFrame(list(map(np.ravel, preds_test)))\n",
    "    nom_, denom_, val_ = calculate_concordance_surv(y_target_val, \n",
    "                                                    y_censoring_val, \n",
    "                                                    preds_test)\n",
    "    print(epoch, val_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance is: 0.49\n"
     ]
    }
   ],
   "source": [
    "preds_test = model_econ(features_val)\n",
    "preds_test = pd.DataFrame(list(map(np.ravel, preds_test)))\n",
    "nom_, denom_, val_ = calculate_concordance_surv(y_target_val, \n",
    "                                                y_censoring_val, \n",
    "                                                preds_test)\n",
    "print(f\"Concordance is: {np.round(val_,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cox = df.drop(columns=['rowid', 'pid'])\n",
    "df_cox['fac_ui'] = np.where(df_cox.fac_ui == 'yes', 1, 0)\n",
    "\n",
    "df_cox_test = df_val.drop(columns=['rowid', 'pid'])\n",
    "df_cox_test['fac_ui'] = np.where(df_cox_test.fac_ui == 'yes', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph = CoxPHFitter()\n",
    "cph.fit(df_cox, duration_col='time', event_col='event')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance is: 0.69\n"
     ]
    }
   ],
   "source": [
    "preds_cox = cph.predict_survival_function(df_cox_test)\n",
    "\n",
    "nom_, denom_, val_ = calculate_concordance_surv(y_target_val, \n",
    "                                                y_censoring_val, \n",
    "                                                preds_cox)\n",
    "print(f\"Concordance is: {np.round(val_,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_example = model_econ(features_example)\n",
    "preds_example = pd.DataFrame(list(map(np.ravel, preds_example)))\n",
    "\n",
    "preds_example.to_csv('data/application/econ_predictions_examples.csv', \n",
    "                     index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tens_env",
   "language": "python",
   "name": "tens_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:21:17) \n[Clang 11.1.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbbf41762ef44221e9bab1cb88f3143b98cae4bbdab9afd61221d8b0b6f6be23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
